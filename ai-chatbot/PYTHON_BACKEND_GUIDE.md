# Python Backend Integration Guide

æœ¬é¡¹ç›®å·²ä¿®æ”¹ä¸ºä½¿ç”¨è‡ªå·±çš„ Python åç«¯æ¥è°ƒç”¨å¤§æ¨¡å‹ï¼Œä¸å†ä¾èµ– AI Gatewayã€‚

## æ¶æ„æ¦‚è§ˆ

```
å‰ç«¯ (Next.js) â†’ Python åç«¯ (FastAPI) â†’ å¤§æ¨¡å‹ API (OpenAI/Claude/etc)
```

## 1. Python åç«¯è®¾ç½®

### å®‰è£…ä¾èµ–

```bash
cd backend  # å‡è®¾ä½ çš„åç«¯åœ¨ backend ç›®å½•
pip install -r requirements.txt
```

### requirements.txt

```
fastapi
uvicorn
openai
anthropic  # å¦‚æœä½¿ç”¨ Claude
pydantic
python-dotenv
```

### åç«¯ä»£ç ç»“æ„

ä½ çš„ Python åç«¯åº”è¯¥æä¾›ä»¥ä¸‹ API endpointï¼š

#### POST /api/chat

**è¯·æ±‚æ ¼å¼:**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "ä½ å¥½"
    }
  ],
  "model": "gpt-4",
  "temperature": 0.7,
  "stream": true
}
```

**å“åº”æ ¼å¼ (æµå¼):**

åç«¯åº”è¯¥è¿”å› SSE (Server-Sent Events) æ ¼å¼çš„æµå¼å“åº”ï¼š

```
0:{"type":"text-delta","textDelta":"ä½ "}
0:{"type":"text-delta","textDelta":"å¥½"}
0:{"type":"text-delta","textDelta":"ï¼"}
d:{"type":"finish","finishReason":"stop"}
```

æ ¼å¼è¯´æ˜ï¼š
- æ¯è¡Œä»¥æ•°å­—å¼€å¤´ï¼ˆ0, d, 9 ç­‰ï¼‰è¡¨ç¤ºä¸åŒçš„æ¶ˆæ¯ç±»å‹
- `0:` è¡¨ç¤ºæ–‡æœ¬å¢é‡
- `d:` è¡¨ç¤ºå®Œæˆä¿¡å·
- `9:` è¡¨ç¤ºå·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæ”¯æŒï¼‰

## 2. å‰ç«¯é…ç½®

### ç¯å¢ƒå˜é‡

åœ¨ `.env.local` æˆ– `.env` æ–‡ä»¶ä¸­è®¾ç½®ï¼š

```bash
# å¼€å‘ç¯å¢ƒ
NEXT_PUBLIC_BACKEND_URL=http://localhost:8000

# ç”Ÿäº§ç¯å¢ƒï¼ˆéƒ¨ç½²åï¼‰
NEXT_PUBLIC_BACKEND_URL=https://your-backend-api.com
```

### æ¨¡å‹æ˜ å°„

å‰ç«¯ä¼šå°†æ¨¡å‹ ID æ˜ å°„åˆ°åç«¯æ”¯æŒçš„æ¨¡å‹ï¼š

```typescript
const modelMapping = {
  "chat-model": "gpt-4",
  "chat-model-reasoning": "gpt-4-turbo-preview",
  "title-model": "gpt-3.5-turbo",
  "artifact-model": "gpt-4",
};
```

ä½ å¯ä»¥åœ¨ `app/(chat)/api/chat/route.ts` ä¸­ä¿®æ”¹è¿™ä¸ªæ˜ å°„ã€‚

## 3. å¯åŠ¨é¡¹ç›®

### å¯åŠ¨åç«¯

```bash
cd backend
python main.py
# æˆ–ä½¿ç”¨ uvicorn
uvicorn main:app --reload --port 8000
```

### å¯åŠ¨å‰ç«¯

```bash
# åœ¨é¡¹ç›®æ ¹ç›®å½•
pnpm dev
```

è®¿é—® http://localhost:3000

## 4. Python åç«¯ç¤ºä¾‹ä»£ç 

### åŸºç¡€å®ç° (OpenAI)

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import openai
import json

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

openai.api_key = "your-api-key"

@app.post("/api/chat")
async def chat(request: dict):
    if request.get("stream"):
        return StreamingResponse(
            stream_openai(request),
            media_type="text/event-stream"
        )

async def stream_openai(request):
    response = await openai.ChatCompletion.acreate(
        model=request["model"],
        messages=request["messages"],
        temperature=request.get("temperature", 0.7),
        stream=True
    )

    async for chunk in response:
        if chunk.choices[0].delta.content:
            data = {
                "type": "text-delta",
                "textDelta": chunk.choices[0].delta.content
            }
            yield f"0:{json.dumps(data)}\n"

    yield f"d:{json.dumps({'type': 'finish', 'finishReason': 'stop'})}\n"
```

### Claude å®ç°

```python
import anthropic

@app.post("/api/chat/claude")
async def chat_claude(request: dict):
    client = anthropic.AsyncAnthropic(api_key="your-api-key")

    if request.get("stream"):
        return StreamingResponse(
            stream_claude(client, request),
            media_type="text/event-stream"
        )

async def stream_claude(client, request):
    async with client.messages.stream(
        model="claude-3-5-sonnet-20241022",
        messages=request["messages"],
        max_tokens=4096,
    ) as stream:
        async for text in stream.text_stream:
            data = {
                "type": "text-delta",
                "textDelta": text
            }
            yield f"0:{json.dumps(data)}\n"

    yield f"d:{json.dumps({'type': 'finish'})}\n"
```

### ä½¿ç”¨æœ¬åœ°æ¨¡å‹ (Ollama)

```python
import ollama

@app.post("/api/chat/local")
async def chat_local(request: dict):
    async def generate():
        stream = ollama.chat(
            model='llama2',
            messages=request["messages"],
            stream=True
        )

        for chunk in stream:
            if chunk['message']['content']:
                data = {
                    "type": "text-delta",
                    "textDelta": chunk['message']['content']
                }
                yield f"0:{json.dumps(data)}\n"

        yield f"d:{json.dumps({'type': 'finish'})}\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

## 5. æ¶ˆæ¯æ ¼å¼è¯´æ˜

### å‰ç«¯å‘é€çš„æ¶ˆæ¯æ ¼å¼

å‰ç«¯ä¼šå°† UI æ¶ˆæ¯è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼å‘é€ç»™åç«¯ï¼š

```typescript
{
  messages: [
    {
      role: "user",
      content: "ç”¨æˆ·çš„æ¶ˆæ¯å†…å®¹"
    },
    {
      role: "assistant",
      content: "AI çš„å›å¤å†…å®¹"
    }
  ],
  model: "gpt-4",
  temperature: 0.7,
  stream: true
}
```

### åç«¯å“åº”æ ¼å¼

æµå¼å“åº”æ¯è¡Œæ ¼å¼ï¼š
- `0:{"type":"text-delta","textDelta":"æ–‡æœ¬å†…å®¹"}` - æ–‡æœ¬å¢é‡
- `d:{"type":"finish","finishReason":"stop"}` - å®Œæˆä¿¡å·

## 6. æ”¯æŒå·¥å…·è°ƒç”¨

å·¥å…·è°ƒç”¨åœ¨ Python åç«¯å®Œæˆï¼Œå‰ç«¯åªè´Ÿè´£æ˜¾ç¤ºå·¥å…·è°ƒç”¨çš„ UIã€‚

### å·¥å…·è°ƒç”¨æµç¨‹

```
ç”¨æˆ·æ¶ˆæ¯ â†’ åç«¯ â†’ LLM (å†³å®šè°ƒç”¨å·¥å…·) â†’ åç«¯æ‰§è¡Œå·¥å…· â†’ è¿”å›ç»“æœ â†’ LLM ç”Ÿæˆæœ€ç»ˆå›å¤ â†’ å‰ç«¯æ˜¾ç¤º
```

### åç«¯å“åº”æ ¼å¼

åç«¯éœ€è¦åœ¨æµå¼å“åº”ä¸­åŒ…å«ä»¥ä¸‹ç±»å‹çš„æ¶ˆæ¯ï¼š

#### 1. å·¥å…·è°ƒç”¨ (Tool Call)
å½“ LLM å†³å®šè°ƒç”¨å·¥å…·æ—¶ï¼Œå‘é€ï¼š
```
9:{"type":"tool-call","toolCallId":"call_abc123","toolName":"get_weather","args":{"location":"Beijing"}}
```

#### 2. å·¥å…·ç»“æœ (Tool Result)
å·¥å…·æ‰§è¡Œå®Œæˆåï¼Œå‘é€ç»“æœï¼š
```
a:{"type":"tool-result","toolCallId":"call_abc123","toolName":"get_weather","result":"Beijing weather: 25Â°C, Sunny"}
```

#### 3. æ–‡æœ¬å“åº” (Text Delta)
æ­£å¸¸çš„æ–‡æœ¬å†…å®¹ï¼š
```
0:{"type":"text-delta","textDelta":"åŒ—äº¬ä»Šå¤©å¤©æ°”æ™´æœ—"}
```

#### 4. å®Œæˆä¿¡å· (Finish)
å“åº”ç»“æŸï¼š
```
d:{"type":"finish","finishReason":"stop"}
```

### Python åç«¯å®ç°ç¤ºä¾‹

```python
import json
from typing import Dict, Any, List
import openai

# å®šä¹‰å·¥å…·
TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "åŸå¸‚åç§°ï¼Œå¦‚ï¼šåŒ—äº¬ã€ä¸Šæµ·"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

# å·¥å…·æ‰§è¡Œå‡½æ•°
def execute_tool(tool_name: str, args: Dict[str, Any]) -> str:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    if tool_name == "get_weather":
        location = args.get("location", "")
        # è¿™é‡Œè°ƒç”¨å®é™…çš„å¤©æ°” API
        return f"{location}å¤©æ°”ï¼šæ™´ï¼Œ25Â°C"
    return "æœªçŸ¥å·¥å…·"

async def stream_with_tools(request: dict):
    """æ”¯æŒå·¥å…·è°ƒç”¨çš„æµå¼å“åº”"""
    messages = request["messages"]
    model = request["model"]

    # ç¬¬ä¸€æ¬¡è°ƒç”¨ LLMï¼Œå¯èƒ½è¿”å›å·¥å…·è°ƒç”¨
    response = await openai.ChatCompletion.acreate(
        model=model,
        messages=messages,
        tools=TOOLS,
        stream=True
    )

    tool_calls = []
    current_tool_call = None

    # å¤„ç†æµå¼å“åº”
    async for chunk in response:
        delta = chunk.choices[0].delta

        # å¤„ç†å·¥å…·è°ƒç”¨
        if hasattr(delta, 'tool_calls') and delta.tool_calls:
            for tool_call_delta in delta.tool_calls:
                if tool_call_delta.index == 0:
                    if current_tool_call is None:
                        current_tool_call = {
                            "id": tool_call_delta.id,
                            "name": "",
                            "arguments": ""
                        }

                    if tool_call_delta.function.name:
                        current_tool_call["name"] += tool_call_delta.function.name

                    if tool_call_delta.function.arguments:
                        current_tool_call["arguments"] += tool_call_delta.function.arguments

        # å¤„ç†æ–‡æœ¬å†…å®¹
        elif hasattr(delta, 'content') and delta.content:
            data = {
                "type": "text-delta",
                "textDelta": delta.content
            }
            yield f"0:{json.dumps(data)}\n"

    # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå¹¶å‘é€ç»“æœ
    if current_tool_call and current_tool_call["name"]:
        tool_call_id = current_tool_call["id"]
        tool_name = current_tool_call["name"]
        tool_args = json.loads(current_tool_call["arguments"])

        # å‘é€å·¥å…·è°ƒç”¨ä¿¡æ¯åˆ°å‰ç«¯
        yield f"9:{json.dumps({
            'type': 'tool-call',
            'toolCallId': tool_call_id,
            'toolName': tool_name,
            'args': tool_args
        })}\n"

        # æ‰§è¡Œå·¥å…·
        tool_result = execute_tool(tool_name, tool_args)

        # å‘é€å·¥å…·ç»“æœåˆ°å‰ç«¯
        yield f"a:{json.dumps({
            'type': 'tool-result',
            'toolCallId': tool_call_id,
            'toolName': tool_name,
            'result': tool_result
        })}\n"

        # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
        messages.append({
            "role": "assistant",
            "tool_calls": [{
                "id": tool_call_id,
                "type": "function",
                "function": {
                    "name": tool_name,
                    "arguments": json.dumps(tool_args)
                }
            }]
        })

        messages.append({
            "role": "tool",
            "tool_call_id": tool_call_id,
            "content": tool_result
        })

        # å†æ¬¡è°ƒç”¨ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
        final_response = await openai.ChatCompletion.acreate(
            model=model,
            messages=messages,
            stream=True
        )

        async for chunk in final_response:
            if chunk.choices[0].delta.content:
                data = {
                    "type": "text-delta",
                    "textDelta": chunk.choices[0].delta.content
                }
                yield f"0:{json.dumps(data)}\n"

    # å‘é€å®Œæˆä¿¡å·
    yield f"d:{json.dumps({'type': 'finish', 'finishReason': 'stop'})}\n"

@app.post("/api/chat")
async def chat(request: dict):
    return StreamingResponse(
        stream_with_tools(request),
        media_type="text/event-stream"
    )
```

### å‰ç«¯æ˜¾ç¤º

å‰ç«¯ä¼šè‡ªåŠ¨è¯†åˆ«å¹¶æ˜¾ç¤ºï¼š
- ğŸ”§ å·¥å…·è°ƒç”¨å¡ç‰‡ï¼ˆæ˜¾ç¤ºå·¥å…·åç§°å’Œå‚æ•°ï¼‰
- ğŸ“‹ å·¥å…·ç»“æœï¼ˆæ˜¾ç¤ºæ‰§è¡Œç»“æœï¼‰
- ğŸ’¬ æœ€ç»ˆçš„æ–‡æœ¬å›å¤

å‰ç«¯ä»£ç å·²ç»é…ç½®ä¸ºç›´æ¥è½¬å‘åç«¯çš„æ‰€æœ‰æ•°æ®ï¼Œæ— éœ€é¢å¤–ä¿®æ”¹ã€‚

## 7. é”™è¯¯å¤„ç†

åç«¯åº”è¯¥æ­£ç¡®å¤„ç†é”™è¯¯å¹¶è¿”å›ï¼š

```python
try:
    # API è°ƒç”¨
    ...
except Exception as e:
    error_data = {
        "type": "error",
        "error": str(e)
    }
    yield f"3:{json.dumps(error_data)}\n"
```

## 8. è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹è¯·æ±‚æ—¥å¿—

åœ¨ `app/(chat)/api/chat/route.ts` ä¸­å·²ç»æ·»åŠ äº†æ—¥å¿—ï¼š

```typescript
console.log('Calling Python backend:', BACKEND_URL);
console.log('Model:', backendModel);
console.log('Messages count:', pythonMessages.length);
```

### æŸ¥çœ‹åç«¯æ—¥å¿—

åœ¨ Python åç«¯æ·»åŠ æ—¥å¿—ï¼š

```python
import logging
logging.basicConfig(level=logging.INFO)

@app.post("/api/chat")
async def chat(request: dict):
    logging.info(f"Received request: model={request['model']}, messages={len(request['messages'])}")
    ...
```

### æµ‹è¯•åç«¯ API

ä½¿ç”¨ curl æµ‹è¯•ï¼š

```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "model": "gpt-4",
    "temperature": 0.7,
    "stream": false
  }'
```

## 9. éƒ¨ç½²å»ºè®®

### åç«¯éƒ¨ç½²

- ä½¿ç”¨ Docker å®¹å™¨åŒ–
- éƒ¨ç½²åˆ° AWS/GCP/Azure
- ä½¿ç”¨ Nginx ä½œä¸ºåå‘ä»£ç†
- é…ç½® HTTPS

### å‰ç«¯éƒ¨ç½²

æ›´æ–°ç¯å¢ƒå˜é‡æŒ‡å‘ç”Ÿäº§åç«¯ï¼š

```bash
NEXT_PUBLIC_BACKEND_URL=https://api.yourdomain.com
```

### CORS é…ç½®

ç”Ÿäº§ç¯å¢ƒæ›´æ–° CORS å…è®¸çš„æºï¼š

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://yourdomain.com",  # ç”Ÿäº§å‰ç«¯åŸŸå
        "http://localhost:3000",    # å¼€å‘ç¯å¢ƒ
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

## 10. å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è‡ªå·±çš„åç«¯ï¼Ÿ

A:
- å®Œå…¨æ§åˆ¶æ¨¡å‹é€‰æ‹©å’Œå‚æ•°
- é¿å… AI Gateway è´¹ç”¨
- å¯ä»¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹
- æ›´çµæ´»çš„å®šåˆ¶

### Q: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„ LLM æä¾›å•†ï¼Ÿ

A: ä¿®æ”¹åç«¯ä»£ç ï¼Œè°ƒç”¨ä¸åŒçš„ APIã€‚å‰ç«¯ä¸éœ€è¦ä¿®æ”¹ã€‚

### Q: æµå¼å“åº”ä¸ºä»€ä¹ˆä¸æ˜¾ç¤ºï¼Ÿ

A:
1. æ£€æŸ¥åç«¯æ˜¯å¦æ­£ç¡®è¿”å› SSE æ ¼å¼
2. ç¡®ä¿ Content-Type æ˜¯ `text/event-stream`
3. æŸ¥çœ‹æµè§ˆå™¨ Network æ ‡ç­¾çš„å“åº”

### Q: å¦‚ä½•æ·»åŠ æ›´å¤šæ¨¡å‹ï¼Ÿ

A: åœ¨ `app/(chat)/api/chat/route.ts` ä¸­æ›´æ–° `modelMapping`ï¼š

```typescript
const modelMapping = {
  "chat-model": "gpt-4o",  // æ”¹ä¸ºå…¶ä»–æ¨¡å‹
  "custom-model": "claude-3-opus-20240229",  // æ·»åŠ æ–°æ¨¡å‹
};
```

## 11. æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜

åœ¨åç«¯æ·»åŠ å“åº”ç¼“å­˜ï¼š

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_response(messages_hash):
    # è¿”å›ç¼“å­˜çš„å“åº”
    pass
```

### é€Ÿç‡é™åˆ¶

```python
from slowapi import Limiter

limiter = Limiter(key_func=get_remote_address)

@app.post("/api/chat")
@limiter.limit("10/minute")
async def chat(request: dict):
    ...
```

## 12. å®‰å…¨å»ºè®®

1. **API Key ç®¡ç†**: ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨ API keys
2. **è¯·æ±‚éªŒè¯**: éªŒè¯è¯·æ±‚æ¥æºå’Œå†…å®¹
3. **é€Ÿç‡é™åˆ¶**: é˜²æ­¢æ»¥ç”¨
4. **HTTPS**: ç”Ÿäº§ç¯å¢ƒå¿…é¡»ä½¿ç”¨ HTTPS
5. **é”™è¯¯ä¿¡æ¯**: ä¸è¦æš´éœ²æ•æ„Ÿçš„é”™è¯¯ä¿¡æ¯

---

ç°åœ¨ä½ å¯ä»¥å®Œå…¨æ§åˆ¶æ•´ä¸ª LLM è°ƒç”¨æµç¨‹äº†ï¼ğŸ‰
